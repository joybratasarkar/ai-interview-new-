# GENERATE_NATURAL_CONVERSATION - LLM CALL ANALYSIS
# Generated: 2025-08-27
# Function: generate_natural_conversation() in natural_interview_flow.py

## FUNCTION PURPOSE
This function generates professional interviewer follow-up responses using psychology tactics and LLM intelligence.

## INPUT PARAMETERS
- question: str - The current interview question
- answer: str - The candidate's answer to analyze
- conversation_context: str - Full conversation history
- role_title: str - Job position being interviewed for
- years_experience: str - Required experience level
- room_id: str - Interview room identifier for caching

## EXECUTION FLOW & LLM CALLS

### EARLY EXIT CONDITIONS (NO LLM CALLS)
1. **Empty Answer Check**
   - IF answer is empty or whitespace only
   - RETURN: "I'd love to hear more about that - could you share some details?"
   - LLM CALLS: 0

2. **Missing Context Check**
   - IF role_title or years_experience is missing
   - RETURN: "That's interesting! Tell me more about that specific aspect."
   - LLM CALLS: 0

### MAIN EXECUTION FLOW (WITH LLM CALLS)

### STEP 1: ANSWER ANALYSIS
**Duration: 1-3 seconds**
**LLM Calls: 2**

#### LLM CALL #1: STAR Completeness Analysis
- **Function**: analyze_star_completeness(answer)
- **Purpose**: Analyze answer for STAR method completeness
- **Input**: Candidate's answer text
- **Output**: {"completeness": 0.0-1.0, "missing_elements": [list]}
- **Prompt**: Analyzes Situation, Task, Action, Result elements
- **Expected Time**: 0.5-1.5 seconds

#### LLM CALL #2: Pattern Detection  
- **Function**: detect_answer_patterns(answer)
- **Purpose**: Detect answer quality patterns and ownership indicators
- **Input**: Candidate's answer text
- **Output**: {"quality_ratio": 0.0-1.0}
- **Prompt**: Analyzes positive/negative indicators, ownership language
- **Expected Time**: 0.5-1.5 seconds

**Step 1 Results Logged:**
- STAR completeness score (0.0-1.0)
- Quality ratio score (0.0-1.0)
- Missing STAR elements list
- Total time for both analyses

### STEP 2: SKILLS CONTEXT RETRIEVAL
**Duration: 0.1-2 seconds (depending on cache)**
**LLM Calls: 0-1 (cache dependent)**

#### Potential LLM Call: Skills Generation
- **Function**: get_key_skills(role_title, years_experience, room_id)
- **Purpose**: Get relevant skills for the role (from cache or generate)
- **Cache Hit**: 0 LLM calls, ~0.1 seconds
- **Cache Miss**: 1 LLM call, ~1-2 seconds
- **Output**: List of 6 key skills for the role
- **Skills Context**: Joined string of skills for prompt context

**Step 2 Results Logged:**
- Skills retrieved (truncated to 100 chars)
- Cache hit/miss status
- Retrieval time

### STEP 3: FOLLOW-UP RESPONSE GENERATION
**Duration: 1-3 seconds**
**LLM Calls: 1**

#### LLM CALL #3: Natural Response Generation
- **Function**: circuit_breaker.call_llm(messages, fallback)
- **Purpose**: Generate intelligent follow-up question
- **Input Context**:
  - Current question and candidate answer
  - Full conversation history
  - STAR analysis results
  - Pattern analysis results
  - Skills context
  - Psychology-based prompting
- **Prompt Strategy**:
  - References specific elements from candidate's answer
  - Uses missing STAR elements to guide follow-up
  - Applies behavioral interviewing techniques
  - Avoids repeating previous questions
- **Expected Time**: 1-3 seconds
- **Output**: Natural follow-up question (15-25 words)

**Prompt Components:**
1. **System Message** (~2000 characters):
   - Role context (interviewer for specific position)
   - Key skills being evaluated
   - Current analysis context (STAR + quality scores)
   - Strict requirements (don't repeat, probe deeper)
   - Follow-up techniques (STAR deepening, specifics, ownership)
   - Example follow-up patterns

2. **Human Message** (~500-1000 characters):
   - Current question
   - Candidate's current answer
   - Full conversation history
   - Analysis results (STAR completeness, quality score, missing elements)
   - Specific task instructions

**Step 3 Results Logged:**
- Prompt length in characters
- Context scores (STAR completeness, quality ratio)
- LLM call success/failure status
- Response generation time
- Generated response (truncated to 100 chars)

## SUCCESS PATH - TOTAL LLM CALLS: 3
1. STAR completeness analysis
2. Pattern detection analysis  
3. Follow-up response generation
**Total Expected Time**: 2-7 seconds (depending on LLM speed and cache status)

## FAILURE PATH - TOTAL LLM CALLS: 2
1. STAR completeness analysis
2. Pattern detection analysis
3. ❌ Follow-up generation fails → Use fallback response
**Fallback Response**: "That's really interesting. Could you tell me more about how you approached that?"

## ERROR HANDLING
- **Exception Caught**: Any error during execution
- **Fallback Response**: "That's a great point. Can you elaborate on that experience?"
- **Logging**: Error details logged to system logger

## OPTIMIZATION OPPORTUNITIES IDENTIFIED

### Current Redundancy Issues:
1. **STAR + Pattern Analysis**: Already computed in calling function but re-computed here
2. **Skills Retrieval**: May cause cache miss if not properly coordinated
3. **Large Prompt Size**: ~2500+ character prompts may be unnecessarily detailed

### Potential Improvements:
1. **Pass Pre-computed Analysis**: Accept STAR/pattern results as parameters to avoid redundant LLM calls
2. **Optimize Prompt Size**: Reduce prompt verbosity while maintaining quality
3. **Cache Coordination**: Better cache key management to ensure hits

## PERFORMANCE METRICS TRACKED
- Individual step timing (Step 1, 2, 3)
- Total LLM call count
- Success/failure rates
- Response quality indicators
- Cache hit/miss ratios

## OUTPUT EXAMPLES
**Successful Follow-up Examples:**
- "You mentioned using Python for data analysis. What specific challenges did you face with that implementation?"
- "That sounds like a complex project. What was YOUR specific role in the architecture decisions?"
- "Interesting approach! How did you measure the success of that solution?"

**Fallback Responses:**
- "That's really interesting. Could you tell me more about how you approached that?"
- "That's a great point. Can you elaborate on that experience?"

This analysis shows the function makes 2-3 LLM calls per invocation, with the potential for optimization by reducing redundant analysis calls.